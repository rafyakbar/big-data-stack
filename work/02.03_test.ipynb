{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896edc36-ba1a-4b31-9d63-600485e53337",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976aef8-a9a8-4232-825e-b82f1dd9ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80f70563-76aa-441c-80bc-732c2f641a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.measure_time(func, *args, **kwargs)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import StringIndexerModel\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "def measure_time(func, *args, **kwargs):\n",
    "    start = time.perf_counter()\n",
    "    result = func(*args, **kwargs)\n",
    "    end = time.perf_counter()\n",
    "    return result, end - start\n",
    "\n",
    "measure_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9cc7d49-99a1-414a-aac4-3ccd3561e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1️⃣ Setup Spark Session\n",
    "# ------------------------------------------------------------\n",
    "# spark, t_spark = measure_time(\n",
    "#     lambda: SparkSession.builder\n",
    "#         .appName(\"RAID-INFERENCE\")\n",
    "#         .master(\"spark://spark-master:7077\")\n",
    "#         .getOrCreate()\n",
    "# )\n",
    "# print(f\"✅ Spark session aktif (waktu: {t_spark:.4f} detik)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17057ebe-caa8-43c9-919f-1a6c84cf8543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session aktif (waktu: 2.2259 detik)\n"
     ]
    }
   ],
   "source": [
    "spark, time_spark = measure_time(\n",
    "    lambda: SparkSession.builder\n",
    "        .appName(\"RAID-INFERENCE\")\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "print(f\"✅ Spark session aktif (waktu: {time_spark:.4f} detik)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a728f09c-ce36-40b2-a995-68cac316f396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Path absolut model: /home/jovyan/work/model-minilm-lr_human-gpt4\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = os.path.abspath(\"model-minilm-lr_human-gpt4\")\n",
    "print(f\"[DEBUG] Path absolut model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812d6196-d283-4c01-a4cd-cdc5f1ca0391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] Memuat model dari lokal: /home/jovyan/work/model-minilm-lr_human-gpt4 ...\n",
      "✅ Model berhasil dimuat dalam 4.1381 detik.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2️⃣ Load Pretrained Model dari Direktori Lokal\n",
    "# ------------------------------------------------------------\n",
    "print(f\"[2] Memuat model dari lokal: {MODEL_PATH} ...\")\n",
    "model, time_model = measure_time(lambda: PipelineModel.load(MODEL_PATH))\n",
    "print(f\"✅ Model berhasil dimuat dalam {time_model:.4f} detik.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4042dba3-989f-4df1-b590-140fef0a36e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] Memuat tokenizer & model MiniLM untuk ekstraksi fitur...\n",
      "✅ MiniLM siap dalam 1.2988 detik (device: cpu).\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3️⃣ Load Sentence Transformer (MiniLM)\n",
    "# ------------------------------------------------------------\n",
    "print(\"[3] Memuat tokenizer & model MiniLM untuk ekstraksi fitur...\")\n",
    "\n",
    "def load_miniLM():\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    encoder = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    encoder.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    encoder.to(device)\n",
    "    return tokenizer, encoder, device\n",
    "\n",
    "(tokenizer, encoder, device), time_minilm = measure_time(load_miniLM)\n",
    "print(f\"✅ MiniLM siap dalam {time_minilm:.4f} detik (device: {device}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "706a2ea8-26f1-43bd-ba62-5d5a9e785602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_embedding(text: str)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4️⃣ Fungsi get_embedding()\n",
    "# ------------------------------------------------------------\n",
    "def get_embedding(text: str):\n",
    "    \"\"\"Ekstraksi embedding 384-dimensi dari teks menggunakan MiniLM\"\"\"\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        return [0.0] * 384\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=512\n",
    "    )\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "    with torch.no_grad():\n",
    "        output = encoder(**encoded)\n",
    "    token_embeddings = output.last_hidden_state\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings.cpu().numpy()[0].tolist()\n",
    "get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b023ab0-31ca-47f1-803a-035dbbe5f680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.classify_text(text: str)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 5️⃣ Fungsi classify_text(text)\n",
    "# ------------------------------------------------------------\n",
    "def classify_text(text: str):\n",
    "    # [a] Embedding\n",
    "    emb, time_emb = measure_time(lambda: get_embedding(text))\n",
    "\n",
    "    # [b] Konversi ke Spark DataFrame\n",
    "    df, time_df = measure_time(lambda: spark.createDataFrame([Row(features=Vectors.dense(emb))]))\n",
    "\n",
    "    # [c] Prediksi\n",
    "    pred_row, time_pred = measure_time(lambda: model.transform(df).select(\"prediction\", \"probability\").collect()[0])\n",
    "\n",
    "    # [d] Interpretasi hasil\n",
    "    label_indexer = model.stages[0]  # StringIndexer pertama dalam pipeline\n",
    "    labels = label_indexer.labels    # contoh: ['gpt4', 'human']\n",
    "\n",
    "    predicted_index = int(pred_row['prediction'])\n",
    "    predicted_label = labels[predicted_index]\n",
    "    prob_vector = pred_row['probability']\n",
    "    prob_dict = {labels[i]: float(prob_vector[i]) for i in range(len(labels))}\n",
    "\n",
    "    time_total = time_emb + time_df + time_pred\n",
    "\n",
    "    return {\n",
    "        \"input_text\": text[:100] + (\"...\" if len(text) > 100 else \"\"),\n",
    "        \"predicted_label\": predicted_label,\n",
    "        \"probabilities\": prob_dict,\n",
    "        \"timing\": {\n",
    "            \"embedding\": round(time_emb, 4),\n",
    "            \"dataframe\": round(time_df, 4),\n",
    "            \"prediction\": round(time_pred, 4),\n",
    "            \"total\": round(time_total, 4)\n",
    "        }\n",
    "    }\n",
    "\n",
    "classify_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4c30348-676e-4150-b120-25eedd87ecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] Menguji teks...\n",
      "✅ Selesai dalam 0.3049 detik\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_text': 'In this paper, we present a semi-supervised learning algorithm for classification of text documents....',\n",
       " 'predicted_label': 'human',\n",
       " 'probabilities': {'gpt4': 0.13300648190988193, 'human': 0.866993518090118},\n",
       " 'timing': {'embedding': 0.0229,\n",
       "  'dataframe': 0.0147,\n",
       "  'prediction': 0.2652,\n",
       "  'total': 0.3028}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6️⃣ Contoh Pengujian\n",
    "# ------------------------------------------------------------\n",
    "sample_text = \"In this paper, we present a semi-supervised learning algorithm for classification of text documents. A method of labeling unlabeled text documents is presented. The presented method is based on the principle of divide and conquer strategy. It uses recursive K-means algorithm for partitioning both labeled and unlabeled data collection. The K-means algorithm is applied recursively on each partition till a desired level partition is achieved such that each partition contains labeled documents of a single class. Once the desired clusters are obtained, the respective cluster centroids are considered as representatives of the clusters and the nearest neighbor rule is used for classifying an unknown text document. Series of experiments have been conducted to bring out the superiority of the proposed model over other recent state of the art models on 20Newsgroups dataset.\"\n",
    "\n",
    "print(\"[6] Menguji teks...\")\n",
    "result, result_time = measure_time(lambda: classify_text(sample_text))\n",
    "print(f\"✅ Selesai dalam {result_time:.4f} detik\\n\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f463d8-1e4a-40f2-b5e1-5a01ff389a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
